{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4yrN+jv3+USgsV0BkiV5O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mike-Wazovsky/JetBrain-emotion-classification/blob/main/other_implementations/JetBrains_other_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **JetBrains**"
      ],
      "metadata": {
        "id": "8t3sZ_PVhXJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOqZFbKahtCJ",
        "outputId": "7f2e1b24-e25a-4772-a821-b7d59068a6ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete this"
      ],
      "metadata": {
        "id": "3YtbNKUf5g7B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vsTj5xyMhDiy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X, y = read_dataset('fb_sentiment.csv')\n",
        "# print(X)\n",
        "# print(y)"
      ],
      "metadata": {
        "id": "ufUXye1XjzJM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stanford CoreNLP parser разобраться что это\n",
        "- The SVM is trained as a one-versus-all\n",
        "classifier with a linear kernel (8 models are trained,\n",
        "one for each emotion of EmoLex) and the TF-IDF\n",
        "model (Salton and Buckley, 1988) is used for providing the input features. Input consists of a single sentence as data (transformed using the TF-IDF model)\n",
        "and an array of 8 values representing the emotions as a\n",
        "label. With a training/test-split of 80%/20%, the average precision-recall is about 0.93."
      ],
      "metadata": {
        "id": "8x717GVbkU-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General Pre-processing \n",
        "def remove_links(tweet):\n",
        "    '''Takes a string and removes web links from it'''\n",
        "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
        "    tweet = tweet.strip('[link]') # remove [links]\n",
        "    return tweet\n",
        "\n",
        "def remove_users(tweet):\n",
        "    '''Takes a string and removes retweet and @user information'''\n",
        "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
        "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
        "    return tweet\n",
        "\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "def preprocess(sent):\n",
        "    sent = remove_users(sent)\n",
        "    sent = remove_links(sent)\n",
        "    sent = sent.lower() # lower case\n",
        "    sent = re.sub('['+my_punctuation + ']+', ' ', sent) # strip punctuation\n",
        "    sent = re.sub('\\s+', ' ', sent) #remove double spacing\n",
        "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
        "    sent_token_list = [word for word in sent.split(' ')]\n",
        "    sent = ' '.join(sent_token_list)\n",
        "    return sent"
      ],
      "metadata": {
        "id": "6df58ywzkUTF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of characters \n",
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "# count number of words \n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# count number of capital characters\n",
        "def count_capital_chars(text):\n",
        "    count=0\n",
        "    for i in text:\n",
        "        if i.isupper():\n",
        "            count+=1\n",
        "    return count\n",
        "\n",
        "# count number of capital words\n",
        "def count_capital_words(text):\n",
        "    return sum(map(str.isupper,text.split()))\n",
        "\n",
        "# count number of punctuations\n",
        "def count_punctuations(text):\n",
        "    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    d=dict()\n",
        "    for i in punctuations:\n",
        "        d[str(i)+' count']=text.count(i)\n",
        "    return d\n",
        "\n",
        "# count number of words in quotes\n",
        "def count_words_in_quotes(text):\n",
        "    x = re.findall(\"\\'.\\'|\\\".\\\"\", text)\n",
        "    count=0\n",
        "    if x is None:\n",
        "        return 0\n",
        "    else:\n",
        "        for i in x:\n",
        "            t=i[1:-1]\n",
        "            count+=count_words(t)\n",
        "        return count\n",
        "    \n",
        "# count number of sentences\n",
        "def count_sent(text):\n",
        "    return len(nltk.sent_tokenize(text))\n",
        "\n",
        "# calculate average word length\n",
        "def avg_word_len(char_cnt,word_cnt):\n",
        "    return char_cnt/word_cnt\n",
        "\n",
        "# calculate average sentence length\n",
        "def avg_sent_len(word_cnt,sent_cnt):\n",
        "    return word_cnt/sent_cnt\n",
        "\n",
        "# count number of unique words \n",
        "def count_unique_words(text):\n",
        "    return len(set(text.split()))\n",
        "            \n",
        "# words vs unique feature\n",
        "def words_vs_unique(words,unique):\n",
        "    return unique/words\n",
        "    \n",
        "# count of hashtags\n",
        "def count_htags(text):\n",
        "    x = re.findall(r'(\\#\\w[A-Za-z0-9]*)', text)\n",
        "    return len(x)\n",
        "\n",
        "# count of mentions\n",
        "def count_mentions(text):\n",
        "    x = re.findall(r'(\\@\\w[A-Za-z0-9]*)', text)\n",
        "    return len(x)\n",
        "\n",
        "# count of stopwords\n",
        "def count_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))  \n",
        "    word_tokens = word_tokenize(text)\n",
        "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
        "    return len(stopwords_x)\n",
        "\n",
        "# stopwords vs words\n",
        "def stopwords_vs_words(stopwords_cnt,text):\n",
        "    return stopwords_cnt/len(word_tokenize(text))"
      ],
      "metadata": {
        "id": "bDPqI3Um6RpQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fb_sentiment.csv')\n",
        "df['char_count'] = df[\"FBPost\"].apply(lambda x:count_chars(x))\n",
        "df['word_count'] = df[\"FBPost\"].apply(lambda x:count_words(x))\n",
        "df['sent_count'] = df[\"FBPost\"].apply(lambda x:count_sent(x))\n",
        "df['capital_char_count'] = df[\"FBPost\"].apply(lambda x:count_capital_chars(x))\n",
        "df['capital_word_count'] = df[\"FBPost\"].apply(lambda x:count_capital_words(x))\n",
        "df['quoted_word_count'] = df[\"FBPost\"].apply(lambda x:count_words_in_quotes(x))\n",
        "df['stopword_count'] = df[\"FBPost\"].apply(lambda x:count_stopwords(x))\n",
        "df['unique_word_count'] = df[\"FBPost\"].apply(lambda x:count_unique_words(x))\n",
        "df['htag_count'] = df[\"FBPost\"].apply(lambda x:count_htags(x))\n",
        "df['mention_count'] = df[\"FBPost\"].apply(lambda x:count_mentions(x))\n",
        "df['avg_wordlength']=df['char_count']/df['word_count']\n",
        "df['avg_sentlength']=df['word_count']/df['sent_count']\n",
        "df['unique_vs_words']=df['unique_word_count']/df['word_count']\n",
        "df['stopwords_vs_words']=df['stopword_count']/df['word_count']\n"
      ],
      "metadata": {
        "id": "8oCtWpY46V2J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size=0.2)"
      ],
      "metadata": {
        "id": "WTFE80C9R6Z7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['FBPost']=train['FBPost'].apply(lambda x: preprocess(x))\n",
        "test['FBPost']=test['FBPost'].apply(lambda x: preprocess(x))"
      ],
      "metadata": {
        "id": "KBJTCBrsBVha"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['FBPost'] = train['FBPost'].astype('string')\n",
        "test['FBPost'] = test['FBPost'].astype('string')\n",
        "train['FBPost'].fillna('', inplace = True)\n",
        "test['FBPost'].fillna('', inplace = True)"
      ],
      "metadata": {
        "id": "8v5uIL0zEp5M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer            =  TfidfVectorizer()\n",
        "train_tf_idf_features =  vectorizer.fit_transform(train['FBPost']).toarray()\n",
        "test_tf_idf_features  =  vectorizer.transform(test['FBPost']).toarray()\n",
        "# Converting above list to DataFrame\n",
        "train_tf_idf          = pd.DataFrame(train_tf_idf_features)\n",
        "test_tf_idf           = pd.DataFrame(test_tf_idf_features)\n",
        "\n",
        "# Saparating train and test labels from all features\n",
        "train_Y               = train['Label']\n",
        "test_Y                = test['Label']\n",
        "# Listing all features\n",
        "features = ['char_count', 'word_count', 'sent_count',\n",
        "       'capital_char_count', 'capital_word_count', 'quoted_word_count',\n",
        "       'stopword_count', 'unique_word_count', 'htag_count', 'mention_count',\n",
        "       'avg_wordlength', 'avg_sentlength', 'unique_vs_words',\n",
        "       'stopwords_vs_words']\n",
        "# Finally merging all features with above TF-IDF. \n",
        "train_fch = train[features]\n",
        "test_fch = test[features]\n",
        "\n",
        "\n",
        "train_fch['Id'] = np.arange(0, len(train_fch))\n",
        "test_fch['Id'] = np.arange(0, len(test_fch))\n",
        "\n",
        "train_tf_idf['Id'] = np.arange(0, len(train_tf_idf))\n",
        "test_tf_idf['Id'] = np.arange(0, len(test_tf_idf))\n",
        "\n",
        "train = train_tf_idf.merge(train_fch, left_on='Id', right_on='Id', how='inner')\n",
        "test = test_tf_idf.merge(test_fch, left_on='Id', right_on='Id', how='inner')\n",
        "\n",
        "train.columns = train.columns.astype(str)\n",
        "test.columns = test.columns.astype(str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12rfwMADC7J1",
        "outputId": "62c29a77-5555-416b-83af-f22079a79909"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-54278f600cab>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_fch['Id'] = np.arange(0, len(train_fch))\n",
            "<ipython-input-10-54278f600cab>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_fch['Id'] = np.arange(0, len(test_fch))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train, train_Y, test_size=0.2, random_state = 42)# Random Forest Classifier\n",
        "RFClassifier = RandomForestClassifier(n_estimators = 1000, min_samples_split = 15, random_state = 42)\n",
        "RFClassifier.fit(X_train, y_train)\n",
        "y_pred = RFClassifier.predict(X_test)\n",
        "val_y_pred = RFClassifier.predict(test)\n",
        "print(\"Accuracy => \", round(accuracy_score(y_pred, y_test)*100, 2))\n",
        "print(\"\\nRandom Forest Classifier results: \\n\")\n",
        "print(classification_report(y_test, y_pred, target_names = ['N', 'O', 'P']))\n",
        "print(\"Validation Accuracy => \", round(accuracy_score(val_y_pred, test_Y)*100, 2))\n",
        "print(\"\\nValidation Random Forest Classifier results: \\n\")\n",
        "print(classification_report(test_Y, val_y_pred, target_names = ['N', 'O', 'P']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0n1lv9BD07z",
        "outputId": "78caa455-2b67-4219-86fe-741a46ce1b6b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy =>  68.75\n",
            "\n",
            "Random Forest Classifier results: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       1.00      0.08      0.14        13\n",
            "           O       0.65      0.25      0.36        44\n",
            "           P       0.69      0.95      0.80       103\n",
            "\n",
            "    accuracy                           0.69       160\n",
            "   macro avg       0.78      0.43      0.43       160\n",
            "weighted avg       0.70      0.69      0.63       160\n",
            "\n",
            "Validation Accuracy =>  67.0\n",
            "\n",
            "Validation Random Forest Classifier results: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.00      0.00      0.00        13\n",
            "           O       0.59      0.17      0.27        58\n",
            "           P       0.68      0.96      0.79       129\n",
            "\n",
            "    accuracy                           0.67       200\n",
            "   macro avg       0.42      0.38      0.35       200\n",
            "weighted avg       0.61      0.67      0.59       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}